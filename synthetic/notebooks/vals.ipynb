{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## {Gaussian, Uniform} to 8-Gaussian, guidance with Learned, MC, CEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from functools import partial\n",
    "from typing import List, Tuple\n",
    "from guided_flow.backbone.mlp import MLP\n",
    "from guided_flow.backbone.wrapper import ExpEnergyMLPWrapper, GuidedMLPWrapper, MLPWrapper\n",
    "from guided_flow.config.sampling import GuideFnConfig\n",
    "from guided_flow.distributions.base import BaseDistribution, get_distribution\n",
    "from guided_flow.distributions.gaussian import GaussianDistribution\n",
    "from guided_flow.flow.optimal_transport import OTPlanSampler\n",
    "from guided_flow.guidance.gradient_guidance import wrap_grad_fn\n",
    "from guided_flow.utils.misc import deterministic\n",
    "from guided_flow.utils.metrics import compute_w2 as w2\n",
    "import torch\n",
    "from torchdyn.core import NeuralODE\n",
    "import numpy as np\n",
    "from torch.distributions import Normal, Independent\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from guided_flow.config.sampling import ODEConfig\n",
    "from fk_steering import evaluate_fk, create_fk_guide_cfg\n",
    "\n",
    "\n",
    "# from guided_flow.utils.kl_divergence import compute_kl_divergence\n",
    "MLP_WIDTH = 256\n",
    "TRAINING_B = 256 # OT CFM training batch size\n",
    "\n",
    "\n",
    "def sample_x1_frompeJ(x1_sampler, x1_dist, device, B):\n",
    "    x1 = None\n",
    "    while x1 is None or x1.shape[0] < B:\n",
    "        x1_ = x1_sampler(B).to(device)\n",
    "        weights = torch.exp(-x1_dist.get_J(x1_))\n",
    "        acc_prob = weights / weights.max()\n",
    "        random_numbers = torch.rand(B, device=device)\n",
    "        x1_ = x1_[random_numbers < acc_prob]\n",
    "        if x1 is None:\n",
    "            x1 = x1_\n",
    "        else:\n",
    "            x1 = torch.cat([x1, x1_], 0)\n",
    "    x1 = x1[:B]\n",
    "    return x1\n",
    "\n",
    "\n",
    "def compute_w2(trajs, cfgs: List[GuideFnConfig]):\n",
    "    w2s = []\n",
    "    for traj, cfg in zip(trajs, cfgs):\n",
    "        x0_dist = get_distribution(cfg.dist_pair[0])\n",
    "        x1_dist = get_distribution(cfg.dist_pair[1])\n",
    "        \n",
    "        x1 = sample_x1_frompeJ(x1_dist.sample, x1_dist, cfg.ode_cfg.device, cfg.ode_cfg.batch_size)\n",
    "        w2s.append(w2(traj[-1], x1))\n",
    "    return w2s\n",
    "\n",
    "def compute_unweighted_w2(trajs, cfgs: List[GuideFnConfig]):\n",
    "    w2s = []\n",
    "    for traj, cfg in zip(trajs, cfgs):\n",
    "        x0_dist = get_distribution(cfg.dist_pair[0])\n",
    "        x1_dist = get_distribution(cfg.dist_pair[1])\n",
    "\n",
    "        x1 = x1_dist.sample(cfg.ode_cfg.batch_size, cfg.ode_cfg.device)\n",
    "        w2s.append(w2(traj[-1], x1))\n",
    "    return w2s\n",
    "\n",
    "\n",
    "def get_mc_guide_fn(x0_dist: BaseDistribution, x1_dist: BaseDistribution, mc_cfg: GuideFnConfig, cfm: str):\n",
    "\n",
    "    def log_cfm_p_t1(x1, xt, t):\n",
    "        # xt = t x1 + (1 - t) x0 -> x0 = xt / (1 - t) - t / (1 - t) x1\n",
    "        x0 = xt / (1 - t + mc_cfg.ep) - (t + mc_cfg.ep) / (1 - t + mc_cfg.ep) * x1 # (B, 2)\n",
    "        p1t = x0_dist.prob(x0).clamp(1e-8) / (1 - t[0] + mc_cfg.ep) ** 2 # (B,)\n",
    "        log_p1t = p1t.log()\n",
    "        # print(log_p1t.mean())\n",
    "        return log_p1t\n",
    "        \n",
    "    def ot_cfm_log_p_tz(x0, x1, xt, t, std=None):\n",
    "        mean = t * x1 + (1 - t) * x0 # (B, 2)\n",
    "         # g.t. std: 0. Too small: requires large mc_batch_size; Too large: inaccurate\n",
    "        base_dist = Normal(loc=mean, scale=std)\n",
    "        distribution = Independent(base_dist, 1)\n",
    "        log_p1t = distribution.log_prob(xt) # (B,)\n",
    "        return log_p1t\n",
    "        \n",
    "    def guide_fn(t, x, dx_dt, model, x0=None, x1=None, Jx1=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            t: Tensor, shape (b, 1)\n",
    "            x: Tensor, shape (b, dim)\n",
    "            dx_dt: Tensor, shape (b, dim)\n",
    "            model: MLP\n",
    "        \"\"\"\n",
    "        # estimate E (e^{-J} / Z - 1) * u\n",
    "        b = x.shape[0]\n",
    "        B = mc_cfg.mc_batch_size\n",
    "        x_ = x.repeat(B, 1) # (MC_B * b, 2)\n",
    "        t_ = t.repeat(B * b, 1) # (MC_B * b)\n",
    "        if cfm == 'cfm':\n",
    "            log_p_t1_x = log_cfm_p_t1(x1, x_, t_) # (MC_B * b) # TODO\n",
    "            log_p_t_x = log_p_t1_x.reshape(B, b, 1).logsumexp(0) - torch.log(torch.tensor(B, device=x.device)) # (MC_B, B, 1) -> (B, 1)\n",
    "            log_p_t1_x_times_J_ = (log_p_t1_x + torch.log(Jx1)).reshape(B, b, 1) # (MC_B * b) -> (MC_B, b, 1)            \n",
    "            logZ = torch.logsumexp(log_p_t1_x_times_J_, 0) - torch.log(torch.tensor(B, device=x.device)) - log_p_t_x # (b, 1)\n",
    "\n",
    "            Z = torch.exp(logZ)\n",
    "            u = (x1 - x_) / (1 - t_ + mc_cfg.ep) # (MC_B * b, dim)\n",
    "\n",
    "            g = (log_p_t1_x.reshape(B, b, 1) - log_p_t_x.unsqueeze(0)).exp() * (Jx1.reshape(B, b, 1) / (Z + 1e-8).unsqueeze(0) - 1) * u.reshape(B, b, 2) # (MC_B, b, dim)\n",
    "\n",
    "            return g.mean(0)\n",
    "        \n",
    "        elif cfm == 'ot_cfm':\n",
    "            log_p_tz_x = ot_cfm_log_p_tz(x0, x1, x_, t_, std=mc_cfg.ot_std) # (MC_B * b)\n",
    "            log_p_t_x = log_p_tz_x.reshape(B, b, 1).logsumexp(0) - torch.log(torch.tensor(B, device=x.device)) # (MC_B, b, 1) -> (b, 1)\n",
    "            log_p_tz_x_times_J_ = (log_p_tz_x + torch.log(Jx1)).reshape(B, b, 1) # (MC_B * b) -> (MC_B, b, 1)\n",
    "            \n",
    "            logZ = torch.logsumexp(log_p_tz_x_times_J_, 0) - torch.log(torch.tensor(B, device=x.device)) - log_p_t_x # (b, 1)\n",
    "            \n",
    "            Z = torch.exp(logZ)\n",
    "            u = x1 - x0 # (MC_B * b, dim)\n",
    "            \n",
    "            g = (log_p_tz_x.reshape(B, b, 1) - log_p_t_x.unsqueeze(0)).exp() * (Jx1.reshape(B, b, 1) / Z - 1) * u.reshape(B, b, 2) # (MC_B, b, dim)\n",
    "            \n",
    "            return g.mean(0)\n",
    "\n",
    "    \n",
    "    if cfm == 'cfm':\n",
    "        x1 = x1_dist.sample(mc_cfg.mc_batch_size).to(mc_cfg.ode_cfg.device).unsqueeze(0).repeat(mc_cfg.ode_cfg.batch_size, 1, 1).permute(1, 0, 2).reshape(-1, 2)\n",
    "        Jx1 = torch.exp(-mc_cfg.scale * x1_dist.get_J(x1))\n",
    "        return partial(\n",
    "            guide_fn, \n",
    "            x1=x1, \n",
    "            Jx1=Jx1\n",
    "        )\n",
    "    elif cfm == 'ot_cfm':\n",
    "        x0 = x0_dist.sample(mc_cfg.mc_batch_size) # (MC_B, 2)\n",
    "        x1 = x1_dist.sample(mc_cfg.mc_batch_size) # (MC_B, 2)\n",
    "        x0_ = x0.to(mc_cfg.ode_cfg.device).unsqueeze(0).repeat(mc_cfg.ode_cfg.batch_size, 1, 1).permute(1, 0, 2).reshape(-1, 2)\n",
    "        x1_ = x1.to(mc_cfg.ode_cfg.device).unsqueeze(0).repeat(mc_cfg.ode_cfg.batch_size, 1, 1).permute(1, 0, 2).reshape(-1, 2)\n",
    "        J_ = torch.exp(-mc_cfg.scale * x1_dist.get_J(x1_)) # (MC_B * b)\n",
    "        \n",
    "        return partial(\n",
    "            guide_fn, \n",
    "            x0=x0_, \n",
    "            x1=x1_, \n",
    "            Jx1=J_\n",
    "        )\n",
    "\n",
    "def get_guide_fn(dist: BaseDistribution, cfg: GuideFnConfig):\n",
    "    def guide_fn(t, x, dx_dt, model):\n",
    "\n",
    "        if cfg.guide_type == 'g_cov_A':\n",
    "            x1_pred = x + dx_dt * (1 - t)\n",
    "            J = dist.get_J(x1_pred)\n",
    "            try:\n",
    "                with torch.enable_grad():\n",
    "                    x1_pred = x1_pred.requires_grad_(True)\n",
    "                    J = dist.get_J(x1_pred)\n",
    "                    grad = -torch.autograd.grad(J.sum(), x1_pred, create_graph=True)[0]\n",
    "                    return grad\n",
    "            except Exception as e:\n",
    "                return torch.zeros_like(x)\n",
    "        \n",
    "        elif cfg.guide_type == 'g_cov_G':\n",
    "            with torch.enable_grad():\n",
    "                x = x.requires_grad_(True)\n",
    "                x1_pred = x + model(torch.cat([x, t.repeat(x.shape[0])[:, None]], 1)) * (1 - t)\n",
    "                J = dist.get_J(x1_pred)\n",
    "                try:\n",
    "                    grad = -torch.autograd.grad(J.sum(), x, create_graph=True)[0]\n",
    "                    return grad\n",
    "                except Exception as e:\n",
    "                    return torch.zeros_like(x)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown guide function: {cfg.guide_type}\")\n",
    "    # make scale and schedule\n",
    "    return wrap_grad_fn(cfg.guide_scale, cfg.guide_schedule, guide_fn)\n",
    "\n",
    "def get_sim_mc_guide_fn(x1_dist: BaseDistribution, cfg: GuideFnConfig):\n",
    "    def guide_fn(t, x, dx_dt, model):\n",
    "        \"\"\"\n",
    "        Implements guidance following Eq. 12\n",
    "        Args:\n",
    "            t: flow time. float\n",
    "            x: current sample x_t. Tensor, shape (b, dim)\n",
    "            dx_dt: current predicted VF. Tensor, shape (b, dim)\n",
    "            model: flow model. MLP\n",
    "        \"\"\"\n",
    "        x1_pred = x + dx_dt * (1 - t) # (B, 2)\n",
    "        std = cfg.sim_mc_std\n",
    "        \n",
    "        x1 = torch.randn_like(x1_pred.unsqueeze(0).repeat(cfg.sim_mc_n, 1, 1)) * std + x1_pred # (cfg.sim_mc_n, B, 2)\n",
    "        Jx1_ = torch.exp(-cfg.scale * x1_dist.get_J(x1.reshape(-1, 2))).reshape(cfg.sim_mc_n, -1) # (cfg.sim_mc_n, B)\n",
    "        v = (x1 - x) / (1 - t + cfg.ep)  # Conditional VF v_{t|z} in Eq. 12 (cfg.sim_mc_n, B, 2)\n",
    "        Z = Jx1_.mean(0) + 1e-8  # Z in Eq. 12 (B,)\n",
    "        g = (Jx1_ / Z - 1).unsqueeze(2) * v  # g in Eq. 12 (cfg.sim_mc_n, B, 2)\n",
    "        return g.mean(0)\n",
    "    return wrap_grad_fn(cfg.guide_scale, cfg.guide_schedule, guide_fn)\n",
    "\n",
    "def evaluate(x0_sampler, x1_sampler, model, guide_fn, cfg: ODEConfig):\n",
    "    node = NeuralODE(\n",
    "        GuidedMLPWrapper(\n",
    "            model, \n",
    "            guide_fn=guide_fn,\n",
    "            scheduler=lambda t: 1\n",
    "        ), \n",
    "        solver=\"euler\", sensitivity=\"adjoint\", atol=1e-4, rtol=1e-4\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        traj = node.trajectory(\n",
    "            x0_sampler(cfg.batch_size).to(cfg.device), \n",
    "            t_span=torch.linspace(0, cfg.t_end, cfg.num_steps)\n",
    "        )\n",
    "    \n",
    "    return traj\n",
    "\n",
    "\n",
    "def sample_and_compute_w2(guide_cfgs: List[GuideFnConfig]):\n",
    "    print(\"Monte Carlo batch size:\", guide_cfgs[0].mc_batch_size)\n",
    "\n",
    "    trajs = []\n",
    "\n",
    "    for cfg in guide_cfgs:\n",
    "\n",
    "        # Initialize samplers, model and guidance model\n",
    "        x0_dist = get_distribution(cfg.dist_pair[0])\n",
    "        x1_dist = get_distribution(cfg.dist_pair[1])\n",
    "\n",
    "        x0_sampler = x0_dist.sample\n",
    "        x1_sampler = x1_dist.sample\n",
    "\n",
    "        model = MLP(dim=2, w=MLP_WIDTH, time_varying=True).to(cfg.ode_cfg.device)\n",
    "        model.load_state_dict(torch.load(f'../logs/{cfg.dist_pair[0]}-{cfg.dist_pair[1]}/{cfg.cfm}_{cfg.dist_pair[0]}_{cfg.dist_pair[1]}/{cfg.cfm}_{cfg.dist_pair[0]}_{cfg.dist_pair[1]}.pth'))\n",
    "\n",
    "        if cfg.guide_type == 'mc':\n",
    "            # sample using flow model\n",
    "            traj = evaluate(x0_sampler, x1_sampler, model, get_mc_guide_fn(x0_dist, x1_dist, cfg, cfg.cfm), cfg.ode_cfg)\n",
    "\n",
    "        elif cfg.guide_type == 'learned':\n",
    "            model_G = MLP(dim=2, out_dim=2, w=MLP_WIDTH, time_varying=True).to(cfg.ode_cfg.device)\n",
    "            model_G.load_state_dict(torch.load(f'../logs/{cfg.dist_pair[0]}-{cfg.dist_pair[1]}/{cfg.cfm}_{cfg.dist_pair[0]}_{cfg.dist_pair[1]}/guidance_matching_{cfg.gm_type}_scale_{cfg.scale}_{cfg.dist_pair[0]}_{cfg.dist_pair[1]}.pth'))\n",
    "            traj = evaluate(x0_sampler, x1_sampler, model, MLPWrapper(model_G, scheduler=lambda t: 1., clamp=0), cfg.ode_cfg)\n",
    "        \n",
    "        elif cfg.guide_type == 'ceg':\n",
    "            model_Z = MLP(dim=2, out_dim=1, w=MLP_WIDTH, time_varying=True, exp_final=False).to(cfg.ode_cfg.device)\n",
    "            model_Z.load_state_dict(torch.load(f'../logs/{cfg.dist_pair[0]}-{cfg.dist_pair[1]}/{cfg.cfm}_{cfg.dist_pair[0]}_{cfg.dist_pair[1]}/ceg_scale_{cfg.scale}_{cfg.dist_pair[0]}_{cfg.dist_pair[1]}.pth'))\n",
    "            \n",
    "            # 2D xy plane. make uniform grid\n",
    "            XX = torch.linspace(0, 1, 100)\n",
    "            YY = torch.linspace(0, 1, 100)\n",
    "            XX, YY = torch.meshgrid(XX, YY, indexing='ij')\n",
    "            xy = torch.stack([XX.flatten(), YY.flatten()], 1)\n",
    "            t = torch.zeros(10000, 1) + 0.9\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.imshow(model_Z(torch.cat([xy, t], 1).to(cfg.ode_cfg.device)).detach().cpu().numpy().reshape(100, 100))\n",
    "            \n",
    "            traj = evaluate(x0_sampler, x1_sampler, model, ExpEnergyMLPWrapper(model_Z, scheduler=lambda t: 1, clamp=1), cfg.ode_cfg)\n",
    "        \n",
    "        elif cfg.guide_type in ['g_cov_A', 'g_cov_G']:\n",
    "            traj = evaluate(x0_sampler, x1_sampler, model, get_guide_fn(x1_dist, cfg), cfg.ode_cfg)\n",
    "\n",
    "        elif cfg.guide_type == 'g_sim_MC':\n",
    "            traj = evaluate(x0_sampler, x1_sampler, model, get_sim_mc_guide_fn(x1_dist, cfg), cfg.ode_cfg)\n",
    "        elif cfg.guide_type == 'fk':\n",
    "            traj = evaluate_fk(x0_sampler, x1_sampler, model, x1_dist, cfg.fk_params, cfg.ode_cfg)\n",
    "        elif cfg.guide_type == 'plain' or cfg.guide_type == 'cfm':\n",
    "            # Plain CFM without any guidance\n",
    "            node = NeuralODE(\n",
    "                MLPWrapper(model, scheduler=lambda t: 1, clamp=0),  # No guidance (scheduler returns 0)\n",
    "                solver=\"euler\", sensitivity=\"adjoint\", atol=1e-4, rtol=1e-4\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                traj = node.trajectory(\n",
    "                    x0_sampler(cfg.ode_cfg.batch_size).to(cfg.ode_cfg.device),\n",
    "                    t_span=torch.linspace(0, cfg.ode_cfg.t_end, cfg.ode_cfg.num_steps)\n",
    "                )\n",
    "\n",
    "        trajs.append(traj)\n",
    "    return trajs, None\n",
    "\n",
    "deterministic(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 40\n",
    "mc_batch_size = 10240\n",
    "disp_traj_batch = 128\n",
    "ode_batch_size = 1024\n",
    "cfm = 'ot_cfm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo batch size: 10240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2333844/3128408257.py:235: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f'../logs/{cfg.dist_pair[0]}-{cfg.dist_pair[1]}/{cfg.cfm}_{cfg.dist_pair[0]}_{cfg.dist_pair[1]}/{cfg.cfm}_{cfg.dist_pair[0]}_{cfg.dist_pair[1]}.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo batch size: 10240\n",
      "Monte Carlo batch size: 10240\n",
      "Monte Carlo batch size: 10240\n",
      "Monte Carlo batch size: 10240\n",
      "Monte Carlo batch size: 10240\n",
      "Monte Carlo batch size: 10240\n",
      "Monte Carlo batch size: 10240\n",
      "Monte Carlo batch size: 10240\n",
      "Monte Carlo batch size: 10240\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.3460798559184829, 0.20576434674641925, 0.10966435191182855],\n",
       " [0.3448578052581991, 0.16637437622492687, 0.11015870516418107],\n",
       " [0.3440059241586379, 0.14976293112519604, 0.14200447072864333],\n",
       " [0.3244642793909614, 0.16452719079974892, 0.1362307332987635],\n",
       " [0.2822554850349835, 0.15478536881468344, 0.11442039089484589],\n",
       " [0.33951585375635973, 0.16739490762591525, 0.13027120088743638],\n",
       " [0.33048232509887676, 0.2415759199582516, 0.12880329751762848],\n",
       " [0.35526426272108613, 0.20571899010862407, 0.11119113909698641],\n",
       " [0.3448609345231764, 0.16501166166307896, 0.11536080346503441],\n",
       " [0.3565914847923446, 0.1600057165067462, 0.12127416283577756]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "num_steps = 40\n",
    "mc_batch_size = 10240\n",
    "disp_traj_batch = 128\n",
    "ode_batch_size = 1024\n",
    "\n",
    "guide_cfgs_mc_cfm = [\n",
    "    GuideFnConfig(cfm=\"cfm\", dist_pair=('circle', 's_curve'), mc_batch_size=mc_batch_size, ep=5e-2, scale=1, ode_cfg=ODEConfig(t_end=1.0, num_steps=num_steps, device='cuda:0', batch_size=ode_batch_size), disp_traj_batch=disp_traj_batch), \n",
    "    GuideFnConfig(cfm=\"cfm\", dist_pair=('uniform', '8gaussian'), mc_batch_size=mc_batch_size, ep=1e-3, ode_cfg=ODEConfig(t_end=1, num_steps=num_steps, device='cuda:0', batch_size=ode_batch_size), disp_traj_batch=disp_traj_batch), \n",
    "    GuideFnConfig(cfm=\"cfm\", dist_pair=('8gaussian', 'moon'), mc_batch_size=mc_batch_size, ep=1e-2, scale=1, ode_cfg=ODEConfig(t_end=1.0, num_steps=num_steps, device='cuda:0', batch_size=ode_batch_size), disp_traj_batch=disp_traj_batch), \n",
    "]\n",
    "deterministic(0)\n",
    "w_2_mc_cfm = []\n",
    "for i in range(10):\n",
    "    trajs_mc_cfm, _ = sample_and_compute_w2(guide_cfgs_mc_cfm)\n",
    "    w_2_mc_cfm.append(compute_w2(trajs_mc_cfm, guide_cfgs_mc_cfm))\n",
    "\n",
    "w_2_mc_cfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33683782 0.17809214 0.12193793]\n",
      "[0.02043313 0.02799803 0.01108433]\n"
     ]
    }
   ],
   "source": [
    "# Generate model wise w2 stats: mean and std\n",
    "w_2_mc_cfm_mean = np.mean(w_2_mc_cfm, axis=0)\n",
    "w_2_mc_cfm_std = np.std(w_2_mc_cfm, axis=0)\n",
    "print(w_2_mc_cfm_mean)\n",
    "print(w_2_mc_cfm_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo batch size: 10240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2333844/3128408257.py:235: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f'../logs/{cfg.dist_pair[0]}-{cfg.dist_pair[1]}/{cfg.cfm}_{cfg.dist_pair[0]}_{cfg.dist_pair[1]}/{cfg.cfm}_{cfg.dist_pair[0]}_{cfg.dist_pair[1]}.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo batch size: 10240\n",
      "Monte Carlo batch size: 10240\n",
      "Monte Carlo batch size: 10240\n",
      "Monte Carlo batch size: 10240\n",
      "Monte Carlo batch size: 10240\n",
      "Monte Carlo batch size: 10240\n",
      "Monte Carlo batch size: 10240\n",
      "Monte Carlo batch size: 10240\n",
      "Monte Carlo batch size: 10240\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.22817266275197415, 0.35572792732769526, 0.23363914126445148],\n",
       " [0.2321455058815066, 0.34102956150330666, 0.22832436719583352],\n",
       " [0.2300536040104048, 0.34544109775856835, 0.213040883054331],\n",
       " [0.262859070630868, 0.3376929216902852, 0.22018501862557827],\n",
       " [0.23093576848866196, 0.33428825630124015, 0.24293387694326773],\n",
       " [0.2726666177453744, 0.35065835867042466, 0.22368135970361785],\n",
       " [0.23124001267907576, 0.34102390660723786, 0.22876696080072315],\n",
       " [0.2587846497327946, 0.35123710064540464, 0.22417950437163864],\n",
       " [0.2746422271186173, 0.3352578834639605, 0.23988095549245136],\n",
       " [0.22147808917793294, 0.3352055758863617, 0.21528866545030392]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "num_steps = 40\n",
    "ode_batch_size = 1024\n",
    "mc_batch_size = 10240\n",
    "cfm = 'ot_cfm'\n",
    "\n",
    "guide_cfgs_mc_ot_cfm = [\n",
    "    GuideFnConfig(cfm=cfm, dist_pair=('circle', 's_curve'), mc_batch_size=mc_batch_size, ep=0.05, ot_std=0.3, scale=1.5, ode_cfg=ODEConfig(t_end=1.0, num_steps=num_steps, device='cuda:0', batch_size=ode_batch_size), disp_traj_batch=disp_traj_batch), \n",
    "    GuideFnConfig(cfm=cfm, dist_pair=('uniform', '8gaussian'), mc_batch_size=mc_batch_size, ep=0.05, ot_std=0.3, scale=2, ode_cfg=ODEConfig(t_end=1, num_steps=num_steps, device='cuda:0', batch_size=ode_batch_size), disp_traj_batch=disp_traj_batch), \n",
    "    GuideFnConfig(cfm=cfm, dist_pair=('8gaussian', 'moon'), mc_batch_size=mc_batch_size, ep=0.05, ot_std=0.3, scale=1.5, ode_cfg=ODEConfig(t_end=1.0, num_steps=num_steps, device='cuda:0', batch_size=ode_batch_size), disp_traj_batch=disp_traj_batch), \n",
    "]\n",
    "\n",
    "deterministic(0)\n",
    "w_2_mc_ot_cfm = []\n",
    "for i in range(10):\n",
    "    trajs_mc_ot_cfm, _ = sample_and_compute_w2(guide_cfgs_mc_ot_cfm)\n",
    "    w_2_mc_ot_cfm.append(compute_w2(trajs_mc_ot_cfm, guide_cfgs_mc_ot_cfm))\n",
    "\n",
    "w_2_mc_ot_cfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24429782 0.34275626 0.22699207]\n",
      "[0.01939162 0.00725538 0.00930691]\n"
     ]
    }
   ],
   "source": [
    "w_2_mc_ot_cfm_mean = np.mean(w_2_mc_ot_cfm, axis=0)\n",
    "w_2_mc_ot_cfm_std = np.std(w_2_mc_ot_cfm, axis=0)\n",
    "\n",
    "print(w_2_mc_ot_cfm_mean)\n",
    "print(w_2_mc_ot_cfm_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo batch size: 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2333844/3128408257.py:235: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f'../logs/{cfg.dist_pair[0]}-{cfg.dist_pair[1]}/{cfg.cfm}_{cfg.dist_pair[0]}_{cfg.dist_pair[1]}/{cfg.cfm}_{cfg.dist_pair[0]}_{cfg.dist_pair[1]}.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo batch size: 1024\n",
      "Monte Carlo batch size: 1024\n",
      "Monte Carlo batch size: 1024\n",
      "Monte Carlo batch size: 1024\n",
      "Monte Carlo batch size: 1024\n",
      "Monte Carlo batch size: 1024\n",
      "Monte Carlo batch size: 1024\n",
      "Monte Carlo batch size: 1024\n",
      "Monte Carlo batch size: 1024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.24362046900326023, 0.07230156491300228, 0.07852409385546293],\n",
       " [0.20053407007942295, 0.11892456916550699, 0.06866183667646909],\n",
       " [0.3007502143289958, 0.12201327052124956, 0.05634350195398302],\n",
       " [0.2610194832097039, 0.20546555961962185, 0.0736289949409307],\n",
       " [0.2772547361363153, 0.10184224138614084, 0.0625664751664994],\n",
       " [0.23237408618076733, 0.0991061503557292, 0.06694504832151671],\n",
       " [0.1460313478144786, 0.11818549975503813, 0.07421850631169574],\n",
       " [0.23729990429175313, 0.13492513413225898, 0.0853091638251044],\n",
       " [0.14647192813321008, 0.2083862172761897, 0.06148638140322131],\n",
       " [0.11330024476641773, 0.1419187616566589, 0.05335496162129663]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples = 128\n",
    "fk_steps = 40\n",
    "\n",
    "guide_cfgs_fk_ot_cfm = [\n",
    "    create_fk_guide_cfg(\n",
    "        cfm=cfm,\n",
    "        dist_pair=('circle', 's_curve'),\n",
    "        scale=1.0,\n",
    "        num_samples=num_samples,\n",
    "        fk_steering_temperature=1.0,\n",
    "        fk_potential_scheduler='harmonic_sum',\n",
    "        resample_method='residual',\n",
    "        resample_freq=10,\n",
    "        ode_cfg=ODEConfig(t_end=1.0, num_steps=fk_steps, batch_size=ode_batch_size)\n",
    "    ),\n",
    "    create_fk_guide_cfg(\n",
    "        cfm=cfm,\n",
    "        dist_pair=('uniform', '8gaussian'),\n",
    "        scale=1.0,\n",
    "        num_samples=num_samples,\n",
    "        resample_freq=10,\n",
    "        fk_potential_scheduler='harmonic_sum',\n",
    "        resample_method='residual',\n",
    "        ode_cfg=ODEConfig(t_end=1.0, num_steps=fk_steps, batch_size=ode_batch_size)\n",
    "    ),\n",
    "    create_fk_guide_cfg(\n",
    "        cfm=cfm,\n",
    "        dist_pair=('8gaussian', 'moon'),\n",
    "        scale=1.0,\n",
    "        num_samples=num_samples,\n",
    "        resample_freq=10,\n",
    "        fk_potential_scheduler='harmonic_sum',\n",
    "        resample_method='residual',\n",
    "        ode_cfg=ODEConfig(t_end=1.0, num_steps=fk_steps, batch_size=ode_batch_size)\n",
    "    ),\n",
    "]\n",
    "\n",
    "deterministic(0)\n",
    "w2s_fk_ot_cfm = []\n",
    "for i in range(10):\n",
    "    trajs_fk_ot_cfm, _ = sample_and_compute_w2(guide_cfgs_fk_ot_cfm)\n",
    "    w2s_fk_ot_cfm.append(compute_w2(trajs_fk_ot_cfm, guide_cfgs_fk_ot_cfm))\n",
    "\n",
    "w2s_fk_ot_cfm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21586565 0.1323069  0.0681039 ]\n",
      "[0.05912884 0.04165931 0.00951033]\n"
     ]
    }
   ],
   "source": [
    "w2s_fk_ot_cfm_mean = np.mean(w2s_fk_ot_cfm, axis=0)\n",
    "w2s_fk_ot_cfm_std = np.std(w2s_fk_ot_cfm, axis=0)\n",
    "print(w2s_fk_ot_cfm_mean)\n",
    "print(w2s_fk_ot_cfm_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo batch size: 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2333844/3128408257.py:235: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f'../logs/{cfg.dist_pair[0]}-{cfg.dist_pair[1]}/{cfg.cfm}_{cfg.dist_pair[0]}_{cfg.dist_pair[1]}/{cfg.cfm}_{cfg.dist_pair[0]}_{cfg.dist_pair[1]}.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo batch size: 1024\n",
      "Monte Carlo batch size: 1024\n",
      "Monte Carlo batch size: 1024\n",
      "Monte Carlo batch size: 1024\n",
      "Monte Carlo batch size: 1024\n",
      "Monte Carlo batch size: 1024\n",
      "Monte Carlo batch size: 1024\n",
      "Monte Carlo batch size: 1024\n",
      "Monte Carlo batch size: 1024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.40087017768182953, 0.09087170984673199, 0.1400530436343675],\n",
       " [0.3015677014565128, 0.08415276742517853, 0.08504342233826034],\n",
       " [0.2982333932800692, 0.12658303113682112, 0.10226961609310374],\n",
       " [0.14082881048252585, 0.13580652382942332, 0.11286135574981801],\n",
       " [0.24138424998973648, 0.17839464811410496, 0.08228535626737513],\n",
       " [0.1832051156059351, 0.14084591583183628, 0.09271955952978851],\n",
       " [0.34441032122005316, 0.12953478200643787, 0.07785058339274747],\n",
       " [0.22474099001279724, 0.17283068883798944, 0.07689874194723346],\n",
       " [0.2826700369365647, 0.18640703894961524, 0.08338107821706077],\n",
       " [0.32143995748576426, 0.12009340895050144, 0.12157328000372469]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples = 128\n",
    "fk_steps = 40\n",
    "\n",
    "guide_cfgs_fk_cfm = [\n",
    "    create_fk_guide_cfg(\n",
    "        cfm=\"cfm\",\n",
    "        dist_pair=('circle', 's_curve'),\n",
    "        scale=1.0,\n",
    "        num_samples=num_samples,\n",
    "        fk_steering_temperature=1.0,\n",
    "        fk_potential_scheduler='harmonic_sum',\n",
    "        resample_freq=10,\n",
    "        resample_method='residual',\n",
    "        ode_cfg=ODEConfig(t_end=1.0, num_steps=fk_steps, batch_size=ode_batch_size)\n",
    "    ),\n",
    "    create_fk_guide_cfg(\n",
    "        cfm=\"cfm\",\n",
    "        dist_pair=('uniform', '8gaussian'),\n",
    "        scale=1.0,\n",
    "        num_samples=num_samples,\n",
    "        resample_freq=10,\n",
    "        fk_potential_scheduler='harmonic_sum',\n",
    "        resample_method='residual',\n",
    "        ode_cfg=ODEConfig(t_end=1.0, num_steps=fk_steps, batch_size=ode_batch_size)\n",
    "    ),\n",
    "    create_fk_guide_cfg(\n",
    "        cfm=\"cfm\",\n",
    "        dist_pair=('8gaussian', 'moon'),\n",
    "        scale=1.0,\n",
    "        num_samples=num_samples,\n",
    "        resample_freq=10,\n",
    "        fk_potential_scheduler='harmonic_sum',\n",
    "        resample_method='residual',\n",
    "        ode_cfg=ODEConfig(t_end=1.0, num_steps=fk_steps, batch_size=ode_batch_size)\n",
    "    ),\n",
    "]\n",
    "deterministic(0)\n",
    "w_2_fk_cfm = []\n",
    "for i in range(10):\n",
    "    trajs_fk_cfm, _ = sample_and_compute_w2(guide_cfgs_fk_cfm)\n",
    "    w_2_fk_cfm.append(compute_w2(trajs_fk_cfm, guide_cfgs_fk_cfm))\n",
    "\n",
    "w_2_fk_cfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.27393508 0.13655205 0.0974936 ]\n",
      "[0.07361731 0.03289885 0.02017019]\n"
     ]
    }
   ],
   "source": [
    "w2s_fk_cfm_mean = np.mean(w_2_fk_cfm, axis=0)\n",
    "w2s_fk_cfm_std = np.std(w_2_fk_cfm, axis=0)\n",
    "print(w2s_fk_cfm_mean)\n",
    "print(w2s_fk_cfm_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo batch size: 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2333844/3128408257.py:235: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f'../logs/{cfg.dist_pair[0]}-{cfg.dist_pair[1]}/{cfg.cfm}_{cfg.dist_pair[0]}_{cfg.dist_pair[1]}/{cfg.cfm}_{cfg.dist_pair[0]}_{cfg.dist_pair[1]}.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo batch size: 1024\n",
      "Monte Carlo batch size: 1024\n",
      "Monte Carlo batch size: 1024\n",
      "Monte Carlo batch size: 1024\n",
      "Monte Carlo batch size: 1024\n",
      "Monte Carlo batch size: 1024\n",
      "Monte Carlo batch size: 1024\n",
      "Monte Carlo batch size: 1024\n",
      "Monte Carlo batch size: 1024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.1332946272797528, 0.1037292227951248, 0.08026145424648462],\n",
       " [0.13385909465190096, 0.11279275045745335, 0.05883949770256176],\n",
       " [0.10095108802602015, 0.1497689558026552, 0.06302064403391057],\n",
       " [0.15157546768029265, 0.11918498726893402, 0.08620107305203592],\n",
       " [0.0862614457078605, 0.15660184235344418, 0.06128817738296346],\n",
       " [0.13309688161928535, 0.10437037930818178, 0.046668999565763855],\n",
       " [0.1167532046886388, 0.1205464729909164, 0.055002711368196545],\n",
       " [0.12472876987364688, 0.19931695673215657, 0.05247170643191792],\n",
       " [0.08016938458809365, 0.16944716700471735, 0.056117664453033685],\n",
       " [0.08323520711676544, 0.09374889342703024, 0.07757159770733393]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guide_cfgs_plain_ot_cfm = [\n",
    "    GuideFnConfig(\n",
    "        cfm=cfm,\n",
    "        dist_pair=('circle', 's_curve'),\n",
    "        guide_type='plain',\n",
    "        scale=1,\n",
    "        ode_cfg=ODEConfig(t_end=1.0, num_steps=100, batch_size=ode_batch_size)\n",
    "    ),\n",
    "    GuideFnConfig(\n",
    "        cfm=cfm,\n",
    "        dist_pair=('uniform', '8gaussian'),\n",
    "        guide_type='plain',\n",
    "        scale=1,\n",
    "        ode_cfg=ODEConfig(t_end=1.0, num_steps=100, batch_size=ode_batch_size)\n",
    "    ),\n",
    "    GuideFnConfig(\n",
    "        cfm=cfm,\n",
    "        dist_pair=('8gaussian', 'moon'),\n",
    "        guide_type='plain',\n",
    "        ode_cfg=ODEConfig(t_end=1.0, num_steps=100, batch_size=ode_batch_size)\n",
    "    ),\n",
    "]\n",
    "\n",
    "deterministic(0)\n",
    "w2s_plain_ot_cfm = []\n",
    "for i in range(10):\n",
    "    trajs_plain_ot_cfm, _ = sample_and_compute_w2(guide_cfgs_plain_ot_cfm)\n",
    "    w2s_plain_ot_cfm.append(compute_unweighted_w2(trajs_plain_ot_cfm, guide_cfgs_plain_ot_cfm))\n",
    "\n",
    "w2s_plain_ot_cfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11439252 0.13295076 0.06374435]\n",
      "[0.02386442 0.03245908 0.01245853]\n"
     ]
    }
   ],
   "source": [
    "w2s_plain_ot_cfm_mean = np.mean(w2s_plain_ot_cfm, axis=0)\n",
    "w2s_plain_ot_cfm_std = np.std(w2s_plain_ot_cfm, axis=0)\n",
    "print(w2s_plain_ot_cfm_mean)\n",
    "print(w2s_plain_ot_cfm_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo batch size: 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2333844/3128408257.py:235: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f'../logs/{cfg.dist_pair[0]}-{cfg.dist_pair[1]}/{cfg.cfm}_{cfg.dist_pair[0]}_{cfg.dist_pair[1]}/{cfg.cfm}_{cfg.dist_pair[0]}_{cfg.dist_pair[1]}.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo batch size: 1024\n",
      "Monte Carlo batch size: 1024\n",
      "Monte Carlo batch size: 1024\n",
      "Monte Carlo batch size: 1024\n",
      "Monte Carlo batch size: 1024\n",
      "Monte Carlo batch size: 1024\n",
      "Monte Carlo batch size: 1024\n",
      "Monte Carlo batch size: 1024\n",
      "Monte Carlo batch size: 1024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.21076680856771404, 0.09270063982362946, 0.07825950258873694],\n",
       " [0.1967778672357857, 0.11839058317260423, 0.08503302750804916],\n",
       " [0.18737991564258183, 0.11449741393163038, 0.09660926688619002],\n",
       " [0.20068317384703638, 0.1214364816265766, 0.0988927008898253],\n",
       " [0.18505342613678893, 0.1553370027766992, 0.09148961575622169],\n",
       " [0.17966549368152743, 0.12395935803802197, 0.07219065128990627],\n",
       " [0.1425727673880871, 0.12344463734612343, 0.08090414180151269],\n",
       " [0.15225909225521192, 0.16950508809332032, 0.08167584471871521],\n",
       " [0.15651903070850776, 0.15717479632183362, 0.07439944696694016],\n",
       " [0.13151833653286352, 0.07815638764748746, 0.07549243267164539]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guide_cfgs_plain_cfm = [\n",
    "    GuideFnConfig(\n",
    "        cfm=\"cfm\",\n",
    "        dist_pair=('circle', 's_curve'),\n",
    "        guide_type='plain',\n",
    "        scale=1,\n",
    "        ode_cfg=ODEConfig(t_end=1.0, num_steps=100, batch_size=1024)\n",
    "    ),\n",
    "    GuideFnConfig(\n",
    "        cfm=\"cfm\",\n",
    "        dist_pair=('uniform', '8gaussian'),\n",
    "        guide_type='plain',\n",
    "        scale=1,\n",
    "        ode_cfg=ODEConfig(t_end=1.0, num_steps=100, batch_size=1024)\n",
    "    ),\n",
    "    GuideFnConfig(\n",
    "        cfm=\"cfm\",\n",
    "        dist_pair=('8gaussian', 'moon'),\n",
    "        guide_type='plain',\n",
    "        ode_cfg=ODEConfig(t_end=1.0, num_steps=100, batch_size=1024)\n",
    "    ),\n",
    "]\n",
    "\n",
    "deterministic(0)\n",
    "w2s_plain_cfm = []\n",
    "for i in range(10):\n",
    "    trajs_plain_cfm, _ = sample_and_compute_w2(guide_cfgs_plain_cfm)\n",
    "    w2s_plain_cfm.append(compute_unweighted_w2(trajs_plain_cfm, guide_cfgs_plain_cfm))\n",
    "\n",
    "w2s_plain_cfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.17431959 0.12546024 0.08349466]\n",
      "[0.02546667 0.02709594 0.00887324]\n"
     ]
    }
   ],
   "source": [
    "w2s_plain_cfm_mean = np.mean(w2s_plain_cfm, axis=0)\n",
    "w2s_plain_cfm_std = np.std(w2s_plain_cfm, axis=0)\n",
    "print(w2s_plain_cfm_mean)\n",
    "print(w2s_plain_cfm_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a csv with the means and stds in the order: mc_cfm, mc_ot_cfm, fk_cfm, fk_ot_cfm (with the three sets each)\n",
    "df = pd.DataFrame({\n",
    "    'method': ['mc_cfm', 'mc_ot_cfm', 'fk_cfm', 'fk_ot_cfm'],\n",
    "    'mean': [w_2_mc_cfm_mean, w_2_mc_ot_cfm_mean, w2s_fk_cfm_mean, w2s_fk_ot_cfm_mean],\n",
    "    'std': [w_2_mc_cfm_std, w_2_mc_ot_cfm_std, w2s_fk_cfm_std, w2s_fk_ot_cfm_std]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('means_stds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guided_flow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
